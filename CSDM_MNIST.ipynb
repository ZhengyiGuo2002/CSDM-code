{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhengyiGuo2002/CSDM-code/blob/main/CSDM_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# ==========================================================\n",
        "# Global config you can set\n",
        "# ==========================================================\n",
        "DATASET_NAME = \"MNIST\"   # e.g., \"MNIST\"; if your torchvision has it, \"QMNIST\", etc.\n",
        "DATA_ROOT    = \"mnist_data\"\n",
        "SIDE_X       = 48        # target side length for uncompressed images (e.g., 40x40)\n",
        "SIDE_Y       = 28        # compressed side length m (i.e., y has M = m*m measurements)\n",
        "SEED         = 1234      # for reproducible A\n",
        "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = 64\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# 0.  Base dataset + uncompressed dataloaders\n",
        "# ==========================================================\n",
        "def build_base_datasets(dataset_name=DATASET_NAME, img_side=SIDE_X, root=DATA_ROOT, download=True):\n",
        "    \"\"\"\n",
        "    Returns torchvision-style train/test datasets that yield tensors in [0,1]\n",
        "    with shape (C, img_side, img_side).\n",
        "    \"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.Resize((img_side, img_side), interpolation=InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor(),  # -> float32 in [0,1], shape (C,H,W)\n",
        "    ])\n",
        "    DatasetClass = getattr(datasets, dataset_name)  # assumes MNIST-like ctor\n",
        "    train_ds = DatasetClass(root=root, train=True,  transform=tfm, download=download)\n",
        "    test_ds  = DatasetClass(root=root, train=False, transform=tfm, download=download)\n",
        "    return train_ds, test_ds\n",
        "\n",
        "def build_uncompressed_loaders(batch_size, dataset_name=DATASET_NAME, img_side=SIDE_X, root=DATA_ROOT, download=True):\n",
        "    train_ds, test_ds = build_base_datasets(dataset_name, img_side, root, download)\n",
        "    tr_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    te_loader = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "    return (train_ds, test_ds), (tr_loader, te_loader)\n",
        "\n",
        "# ==========================================================\n",
        "# 1.  COMPRESS – Gaussian sketch & dataset wrapper\n",
        "# ==========================================================\n",
        "def _infer_dims(ds):\n",
        "    x0, _ = ds[0]\n",
        "    assert x0.ndim == 3 and x0.shape[1] == x0.shape[2], \"Dataset must yield (C,H,W) with square H==W.\"\n",
        "    C, H, W = x0.shape\n",
        "    D = C * H * W\n",
        "    return C, H, W, D\n",
        "\n",
        "def _make_A(D, M, seed=SEED):\n",
        "    \"\"\"\n",
        "    Gaussian sketch A ~ N(0, 1/M) with shape (M, D).\n",
        "    \"\"\"\n",
        "    g = torch.Generator(device=\"cpu\").manual_seed(seed) if seed is not None else None\n",
        "    A = torch.normal(mean=0.0, std=1.0 / math.sqrt(M), size=(M, D), generator=g)\n",
        "    return A  # keep on CPU; move to GPU only if you need to multiply there\n",
        "\n",
        "class CompressedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps a base image dataset (yielding (C,H,W), label) into compressed measurements.\n",
        "\n",
        "    Returns per item:\n",
        "        y_vec : (M,)            # flat measurement vector, M = m_side * m_side\n",
        "        y_img : (1, m_side, m_side)  # square view of y_vec for image diffusers\n",
        "        label : original label\n",
        "    \"\"\"\n",
        "    def __init__(self, base_ds, A, m_side: int):\n",
        "        super().__init__()\n",
        "        self.ds = base_ds\n",
        "        self.A = A  # (M, D) on CPU\n",
        "        self.m_side = m_side\n",
        "        self.M = m_side * m_side\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_img, lbl = self.ds[idx]            # x_img: (C,S,S) in [0,1]\n",
        "        x = x_img.reshape(-1)                # (D,)\n",
        "        # y = A x\n",
        "        # (both A and x are CPU tensors here; if you need GPU matmul later, move both to the same device)\n",
        "        y = torch.mv(self.A, x)              # (M,)\n",
        "        y_img = y.view(1, self.m_side, self.m_side)\n",
        "        return y, y_img, lbl\n",
        "\n",
        "def build_compressed_loaders(train_ds, test_ds, batch_size, m_side=SIDE_Y, seed=SEED):\n",
        "    \"\"\"\n",
        "    Given base (uncompressed) datasets, build a single fixed Gaussian A and\n",
        "    dataloaders for:\n",
        "      - vector measurements (for 1D diffusion)\n",
        "      - image-shaped measurements (for 2D diffusion)\n",
        "    \"\"\"\n",
        "    C, H, W, D = _infer_dims(train_ds)\n",
        "    M = m_side * m_side\n",
        "    assert M <= D, f\"Compressed dimension M={M} should be <= D={D} for a true compression.\"\n",
        "\n",
        "    A = _make_A(D, M, seed=seed)  # CPU\n",
        "    comp_train = CompressedDataset(train_ds, A, m_side)\n",
        "    comp_test  = CompressedDataset(test_ds,  A, m_side)\n",
        "\n",
        "    # Collate for vector (B, M) + labels\n",
        "    def collate_vec(batch):\n",
        "        y_vec = torch.stack([b[0] for b in batch], dim=0)                 # (B, M)\n",
        "        lbl   = torch.tensor([b[2] for b in batch], dtype=torch.long)\n",
        "        return y_vec, lbl\n",
        "\n",
        "    # Collate for image view (B, 1, m, m) + labels\n",
        "    def collate_img(batch):\n",
        "        y_img = torch.stack([b[1] for b in batch], dim=0)                 # (B, 1, m, m)\n",
        "        lbl   = torch.tensor([b[2] for b in batch], dtype=torch.long)\n",
        "        return y_img, lbl\n",
        "\n",
        "    # Vector dataloaders (for 1D diffusion)\n",
        "    tr_loader_vec = DataLoader(comp_train, batch_size=batch_size, shuffle=True,  collate_fn=collate_vec)\n",
        "    te_loader_vec = DataLoader(comp_test,  batch_size=batch_size, shuffle=False, collate_fn=collate_vec)\n",
        "\n",
        "    # Image dataloaders (for 2D diffusion)\n",
        "    tr_loader_img = DataLoader(comp_train, batch_size=batch_size, shuffle=True,  collate_fn=collate_img)\n",
        "    te_loader_img = DataLoader(comp_test,  batch_size=batch_size, shuffle=False, collate_fn=collate_img)\n",
        "\n",
        "    return A, (tr_loader_vec, te_loader_vec), (tr_loader_img, te_loader_img)\n",
        "\n",
        "# ==========================================================\n",
        "# Example usage (keeps your original variable names where sensible)\n",
        "# ==========================================================\n",
        "# 0) build uncompressed loaders (no compression)\n",
        "# Note: use your existing `batch` variable\n",
        "(train_ds_raw, test_ds_raw), (tr_loader, te_loader) = build_uncompressed_loaders(\n",
        "    batch_size=batch,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    img_side=SIDE_X,\n",
        "    root=DATA_ROOT,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# 1) build compressed loaders (vector + image views) using one fixed A\n",
        "A_cpu, (tr_loader_c_vec, te_loader_c_vec), (tr_loader_c_img, te_loader_c_img) = build_compressed_loaders(\n",
        "    train_ds=train_ds_raw,\n",
        "    test_ds=test_ds_raw,\n",
        "    batch_size=batch,\n",
        "    m_side=SIDE_Y,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "# (Optional) have a GPU copy for later ops if needed\n",
        "A_gpu = A_cpu.to(device)"
      ],
      "metadata": {
        "id": "YBHXv5fL5l3R",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T16:29:39.698071Z",
          "iopub.execute_input": "2025-08-29T16:29:39.698296Z",
          "iopub.status.idle": "2025-08-29T16:29:58.135655Z",
          "shell.execute_reply.started": "2025-08-29T16:29:39.698274Z",
          "shell.execute_reply": "2025-08-29T16:29:58.134970Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion"
      ],
      "metadata": {
        "id": "NZLBJGU3VOJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "class RunningNormalizer:\n",
        "    \"\"\"\n",
        "    Simple scalar normalizer: x_norm = (x - mean) / (std + eps).\n",
        "    Use one global mean/std for stability in compressed domain.\n",
        "    \"\"\"\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.eps = eps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fit_from_loader(self, loader, is_vector: bool, device: torch.device, max_batches: Optional[int] = 50):\n",
        "        \"\"\"\n",
        "        Estimate global mean/std from (up to) max_batches of the loader.\n",
        "        - vector loader: yields (y_vec, lbl)\n",
        "        - image loader : yields (y_img, lbl); will flatten to vectors for scale stats\n",
        "        \"\"\"\n",
        "        cnt = 0\n",
        "        s1, s2, n = 0.0, 0.0, 0\n",
        "        for b, (y, _) in enumerate(loader):\n",
        "            y = y.to(device=device, dtype=torch.float32)\n",
        "            if not is_vector:  # image: (B,1,m,m)\n",
        "                y = y.view(y.size(0), -1)\n",
        "            s1 += y.sum().item()\n",
        "            s2 += (y ** 2).sum().item()\n",
        "            n  += y.numel()\n",
        "            cnt += 1\n",
        "            if max_batches is not None and cnt >= max_batches:\n",
        "                break\n",
        "        mean = s1 / max(1, n)\n",
        "        var  = s2 / max(1, n) - mean * mean\n",
        "        std  = math.sqrt(max(var, 1e-8))\n",
        "        self.mean = mean\n",
        "        self.std  = std\n",
        "\n",
        "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return (x - self.mean) / (self.std + self.eps)\n",
        "\n",
        "    def denormalize(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x * (self.std + self.eps) + self.mean\n",
        "\n",
        "# ==========================================================\n",
        "# SDE definitions (VE, VP) & marginals\n",
        "# ==========================================================\n",
        "class VPSDE:\n",
        "    \"\"\"\n",
        "    Variance-Preserving SDE (DDPM continuous limit):\n",
        "        dX = -0.5 β(t) X dt + sqrt(β(t)) dW\n",
        "    Marginal: X_t = α(t) X_0 + σ(t) ε\n",
        "      with α(t) = exp(-0.5 ∫ β),  σ^2(t) = 1 - α(t)^2\n",
        "    \"\"\"\n",
        "    def __init__(self, beta_min=0.1, beta_max=20.0):\n",
        "        self.beta_min = float(beta_min)\n",
        "        self.beta_max = float(beta_max)\n",
        "        self.delta = self.beta_max - self.beta_min\n",
        "\n",
        "    def beta(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        # linear from beta_min to beta_max over t in [0,1]\n",
        "        return self.beta_min + t * self.delta\n",
        "\n",
        "    def a(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        # α(t) = exp(-0.5 * (beta_min t + 0.5 delta t^2))\n",
        "        return torch.exp(-0.5 * (self.beta_min * t + 0.5 * self.delta * t * t))\n",
        "\n",
        "    def sigma(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        a = self.a(t)\n",
        "        return torch.sqrt(torch.clamp(1.0 - a * a, min=1e-10))\n",
        "\n",
        "    def g2(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        # g^2(t) = β(t)\n",
        "        return self.beta(t)\n",
        "\n",
        "    def f(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "        # f(t,x) = -0.5 β(t) x\n",
        "        beta_t = self.beta(t).view(-1, *([1] * (x.dim() - 1)))\n",
        "        return -0.5 * beta_t * x\n",
        "\n",
        "    def prior_sample(self, shape, device):\n",
        "        # X_T ~ N(0, I)\n",
        "        return torch.randn(shape, device=device)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# Losses (DSM): target score = -epsilon / sigma(t)\n",
        "# ==========================================================\n",
        "def dsm_loss(model, x0, sde, t, weight_type=\"sigma2\", is_image: bool = False):\n",
        "    \"\"\"\n",
        "    x0: (B, M) for vectors OR (B,1,m,m) for images\n",
        "    sde: VESDE or VPSDE\n",
        "    t:  (B,) uniform in (eps,1]\n",
        "    weight_type: \"sigma2\" (λ(t)=σ^2(t)) or \"none\"\n",
        "    returns: scalar loss\n",
        "    \"\"\"\n",
        "    eps = torch.randn_like(x0)\n",
        "\n",
        "    # VP\n",
        "    alpha_t = sde.a(t).view(-1, *([1] * (x0.dim()-1)))\n",
        "    sigma_t = sde.sigma(t).view(-1, *([1] * (x0.dim()-1)))\n",
        "    xt = alpha_t * x0 + sigma_t * eps\n",
        "    target = - eps / sigma_t\n",
        "\n",
        "    pred = model(xt, t)\n",
        "    if weight_type == \"sigma2\":\n",
        "        w = (sigma_t ** 2)\n",
        "        loss = (((pred - target) ** 2) * w).mean()\n",
        "    else:\n",
        "        loss = ((pred - target) ** 2).mean()\n",
        "    return loss\n",
        "\n",
        "# ==========================================================\n",
        "# Training loops\n",
        "# ==========================================================\n",
        "def train_score_img(\n",
        "    tr_loader_c_img, te_loader_c_img,\n",
        "    device: torch.device,\n",
        "    sde_type: str = \"VE\",\n",
        "    sigma_min: float = 0.01, sigma_max: float = 50.0,  # VE\n",
        "    beta_min: float = 0.1,  beta_max: float = 20.0,    # VP\n",
        "    lr: float = 2e-4, epochs: int = 10,\n",
        "    weight_type: str = \"sigma2\",\n",
        "    normalize: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a score model on compressed images (B, 1, m, m).\n",
        "    \"\"\"\n",
        "    normalizer = RunningNormalizer()\n",
        "    if normalize:\n",
        "        normalizer.fit_from_loader(tr_loader_c_img, is_vector=False, device=device, max_batches=300)\n",
        "\n",
        "    if sde_type.upper() == \"VE\":\n",
        "        sde = VESDE(sigma_min=sigma_min, sigma_max=sigma_max)\n",
        "    else:\n",
        "        sde = VPSDE(beta_min=beta_min, beta_max=beta_max)\n",
        "\n",
        "    model = UNetScoreNetImg(base_ch=128, tdim=256).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # NEW: ReduceLROnPlateau scheduler (reacts to validation loss)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        opt, mode='min', factor=0.5, patience=2, threshold=1e-4,\n",
        "        cooldown=1, min_lr=2e-6\n",
        "    )\n",
        "\n",
        "    for ep in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        run_loss, nsteps = 0.0, 0\n",
        "        for y_img, _ in tr_loader_c_img:\n",
        "            y_img = y_img.to(device=device, dtype=torch.float32)\n",
        "            if normalize:\n",
        "                y_img = normalizer.normalize(y_img)\n",
        "\n",
        "            t = torch.rand(y_img.size(0), device=device) * (1.0 - 1e-5) + 1e-5\n",
        "            loss = dsm_loss(model, y_img, sde, t, weight_type=weight_type, is_image=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            run_loss += float(loss.item()); nsteps += 1\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, vsteps = 0.0, 0\n",
        "            for y_img_val, _ in te_loader_c_img:\n",
        "                y_img_val = y_img_val.to(device=device, dtype=torch.float32)\n",
        "                if normalize:\n",
        "                    y_img_val = normalizer.normalize(y_img_val)\n",
        "                t_val = torch.rand(y_img_val.size(0), device=device) * (1.0 - 1e-5) + 1e-5\n",
        "                loss_val = dsm_loss(model, y_img_val, sde, t_val,\n",
        "                                    weight_type=weight_type, is_image=True)\n",
        "                val_loss += float(loss_val.item()); vsteps += 1\n",
        "            val_loss /= max(1, vsteps)\n",
        "\n",
        "        # NEW: step the scheduler with validation loss\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"[Img][{sde_type}] epoch {ep+1}/{epochs}  \"\n",
        "              f\"train_loss={run_loss/max(1,nsteps):.4f}  val_loss={val_loss:.4f}  lr={current_lr:.2e}\")\n",
        "\n",
        "    return model, sde, normalizer\n",
        "\n",
        "# ==========================================================\n",
        "# Sampling (Probability-flow ODE; deterministic)\n",
        "# ==========================================================\n",
        "@torch.no_grad()\n",
        "def sample_compressed_img(\n",
        "    model, sde, normalizer, num_samples, m_side, device,\n",
        "    steps=1000, snr=0.15, use_pc=True\n",
        "):\n",
        "    \"\"\"Return (N,1,side,side) in compressed or image space (取决于训练域)。\"\"\"\n",
        "    model.eval()\n",
        "    x = sde.prior_sample((num_samples, 1, m_side, m_side), device=device)  # t=1\n",
        "\n",
        "    t_grid = torch.linspace(1.0, 0.0, steps+1, device=device)\n",
        "    for i in range(steps):\n",
        "        t, t_next = t_grid[i], t_grid[i+1]\n",
        "        dt = (t_next - t)  # 负数\n",
        "        tb = torch.full((num_samples,), float(t.item()), device=device)\n",
        "\n",
        "        s = model(x, tb)  # score(x,t)\n",
        "\n",
        "        beta = sde.g2(tb).view(-1,1,1,1)          # = β(t)\n",
        "        drift = -0.5 * beta * x - beta * s\n",
        "        diff  = torch.sqrt(torch.clamp(beta * (-dt), min=1e-12))\n",
        "\n",
        "        # Predictor: Euler–Maruyama\n",
        "        z = torch.randn_like(x)\n",
        "        x = x + drift * dt + diff * z\n",
        "\n",
        "        if use_pc:\n",
        "            # Corrector: Langevin steps（1~2 步通常够用）\n",
        "            with torch.no_grad():\n",
        "                # 自适应步长：基于目标 SNR\n",
        "                noise_std = diff.mean()\n",
        "                score_norm = torch.sqrt((s**2).mean(dim=(1,2,3), keepdim=True) + 1e-12)\n",
        "                step_size = (snr * noise_std / (score_norm + 1e-12))**2\n",
        "                for _ in range(1):  # 1次校正\n",
        "                    zc = torch.randn_like(x)\n",
        "                    x = x + step_size * s + torch.sqrt(2.0 * step_size) * zc\n",
        "\n",
        "    if normalizer.mean is not None:\n",
        "        x = normalizer.denormalize(x)\n",
        "    return x.clamp(-1, 1)\n",
        "\n",
        "# ==========================================================\n",
        "# Decoding sampled compressed data with your FISTA\n",
        "# ==========================================================\n",
        "@torch.no_grad()\n",
        "def reconstruct_from_compressed_vectors(\n",
        "    y_vec: torch.Tensor,        # (N, M)\n",
        "    A: torch.Tensor,            # (M, D)  on SAME device as y_vec or will be moved\n",
        "    img_shape: Tuple[int,int,int],  # (C,H,W)\n",
        "    lam: float,\n",
        "    L: Optional[float] = None,\n",
        "    max_iter: int = 500,\n",
        "    tol: float = 1e-5,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Uses your fista_lasso_batch to decode y -> x in image space.\n",
        "    \"\"\"\n",
        "    device = A.device\n",
        "    y_vec = y_vec.to(device=device, dtype=A.dtype)\n",
        "    X_hat = fista_lasso_batch(A, y_vec, lam=lam, L=L, max_iter=max_iter, tol=tol)  # (N, D)\n",
        "    with torch.no_grad():\n",
        "        frac_clipped = ((X_hat < 0) | (X_hat > 1)).float().mean().item()\n",
        "        print(f\"clipped fraction: {frac_clipped:.3f}\")\n",
        "    C,H,W = img_shape\n",
        "    X_hat = X_hat.view(-1, C, H, W).clamp(0,1).cpu()\n",
        "    return X_hat\n",
        "\n",
        "@torch.no_grad()\n",
        "def reconstruct_from_compressed_images(\n",
        "    y_img: torch.Tensor,        # (N, 1, m, m)\n",
        "    A: torch.Tensor,            # (M, D)\n",
        "    img_shape: Tuple[int,int,int],\n",
        "    lam: float,\n",
        "    L: Optional[float] = None,\n",
        "    max_iter: int = 500,\n",
        "    tol: float = 1e-5,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Flatten y_img and decode with FISTA.\n",
        "    \"\"\"\n",
        "    y_vec = y_img.view(y_img.size(0), -1)\n",
        "    return reconstruct_from_compressed_vectors(y_vec, A, img_shape, lam, L, max_iter, tol)"
      ],
      "metadata": {
        "id": "sC0GxKYEU3l5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T16:29:58.136842Z",
          "iopub.execute_input": "2025-08-29T16:29:58.137197Z",
          "iopub.status.idle": "2025-08-29T16:29:58.165215Z",
          "shell.execute_reply.started": "2025-08-29T16:29:58.137177Z",
          "shell.execute_reply": "2025-08-29T16:29:58.164607Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Minimal, Drop-in Better UNet for compressed \"images\" =====\n",
        "# Implements:\n",
        "# (1) Upsample+Conv (no transposed conv -> fewer checkerboard artifacts)\n",
        "# (2) 1x1 conv for channel alignment (no F.pad)\n",
        "# (3) FiLM time conditioning (scale-shift after norm)\n",
        "# (4) Two down/up scales + lightweight 2D attention at bottleneck\n",
        "# (5) Wider base channels + zero-inited output head\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# --- (Aux) Sinusoidal time embedding ---\n",
        "class SinusoidalTimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        t: (B,) in [0, 1] or [0, T]\n",
        "        returns: (B, dim)\n",
        "        \"\"\"\n",
        "        half = self.dim // 2\n",
        "        # Use log frequencies\n",
        "        emb_scale = torch.exp(\n",
        "            torch.linspace(math.log(1.0), math.log(10000.0), half, device=t.device)\n",
        "        )\n",
        "        # shape: (B, half)\n",
        "        args = t[:, None] * emb_scale[None, :]\n",
        "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "        if self.dim % 2 == 1:\n",
        "            emb = F.pad(emb, (0, 1))\n",
        "        return emb\n",
        "\n",
        "\n",
        "# --- (Aux) helpers ---\n",
        "def zero_module(m: nn.Module) -> nn.Module:\n",
        "    \"\"\"Zero-initialize a module's parameters (useful for output/layer ends).\"\"\"\n",
        "    for p in m.parameters():\n",
        "        nn.init.zeros_(p)\n",
        "    return m\n",
        "\n",
        "def _gn_groups(c: int, max_groups: int = 32) -> int:\n",
        "    \"\"\"Pick a GroupNorm groups count that divides c (<= max_groups).\"\"\"\n",
        "    for g in range(min(max_groups, c), 0, -1):\n",
        "        if c % g == 0: return g\n",
        "    return 1\n",
        "\n",
        "\n",
        "class Skip1x1(nn.Module):\n",
        "    \"\"\"Channel alignment 1x1 conv (replaces F.pad).\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Simple stride-2 downsample.\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int):\n",
        "        super().__init__()\n",
        "        self.op = nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Nearest-neighbor upsample + 3x3 conv (replaces ConvTranspose2d).\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int):\n",
        "        super().__init__()\n",
        "        self.ups = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.conv(self.ups(x))\n",
        "\n",
        "\n",
        "class SelfAttention2d(nn.Module):\n",
        "    \"\"\"Lightweight MHSA on 2D feature maps. Run it only at small resolution.\"\"\"\n",
        "    def __init__(self, channels: int, num_heads: int = 4):\n",
        "        super().__init__()\n",
        "        assert channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = channels // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Conv2d(channels, 3 * channels, 1)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "        # Optional norm before attention (kept simple & robust)\n",
        "        self.norm = nn.GroupNorm(_gn_groups(channels), channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.qkv(x)  # (B, 3C, H, W)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=1)  # each (B, C, H, W)\n",
        "\n",
        "        # reshape to (B, nH, HW, head_dim)\n",
        "        def _reshape(t):\n",
        "            return t.view(b, self.num_heads, self.head_dim, h * w).permute(0, 1, 3, 2).contiguous()\n",
        "        q, k, v = map(_reshape, (q, k, v))\n",
        "\n",
        "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) * self.scale, dim=-1)  # (B, nH, HW, HW)\n",
        "        out = torch.matmul(attn, v)  # (B, nH, HW, head_dim)\n",
        "\n",
        "        out = out.permute(0, 1, 3, 2).contiguous().view(b, c, h, w)  # (B, C, H, W)\n",
        "        out = self.proj(out)\n",
        "        return x_in + out\n",
        "\n",
        "\n",
        "class ResBlockFiLM(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet block with FiLM time conditioning: scale-shift after norm\n",
        "    Replaces your original ResBlock (which added a bias from temb)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, tdim: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        g1 = _gn_groups(in_ch)\n",
        "        g2 = _gn_groups(out_ch)\n",
        "\n",
        "        self.norm1 = nn.GroupNorm(g1, in_ch)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(g2, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        # FiLM gamma/beta from time embedding\n",
        "        self.emb = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(tdim, 2 * out_ch)\n",
        "        )\n",
        "\n",
        "        self.skip = Skip1x1(in_ch, out_ch)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Zero-init the last conv for stability\n",
        "        self.conv2 = zero_module(self.conv2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, temb: torch.Tensor) -> torch.Tensor:\n",
        "        # First conv\n",
        "        h = self.conv1(F.silu(self.norm1(x)))\n",
        "\n",
        "        # FiLM conditioning on the second normed activations\n",
        "        gamma, beta = self.emb(temb).chunk(2, dim=1)  # (B, C), (B, C)\n",
        "        h = self.norm2(h)\n",
        "        h = (1 + gamma[:, :, None, None]) * h + beta[:, :, None, None]\n",
        "        h = self.dropout(F.silu(h))\n",
        "\n",
        "        h = self.conv2(h)\n",
        "        return self.skip(x) + h\n",
        "\n",
        "\n",
        "class UNetScoreNetImg(nn.Module):\n",
        "    \"\"\"\n",
        "    Small-but-strong UNet for 1x(HxW) compressed 'images':\n",
        "    - two downs/ups\n",
        "    - FiLM time conditioning\n",
        "    - bottleneck attention\n",
        "    - zero-inited output head\n",
        "    NOTE: Expect H and W divisible by 4 (e.g., 28 -> 14 -> 7).\n",
        "    \"\"\"\n",
        "    def __init__(self, base_ch: int = 64, tdim: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.time_emb = SinusoidalTimeEmbedding(tdim)\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(tdim, 4 * tdim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(4 * tdim, tdim),\n",
        "        )\n",
        "\n",
        "        # Encoder blocks (每尺度两个 block)\n",
        "        self.in_conv = nn.Conv2d(1, base_ch, 3, padding=1)\n",
        "        self.e1  = ResBlockFiLM(base_ch, base_ch, tdim, dropout=dropout)\n",
        "        self.e1b = ResBlockFiLM(base_ch, base_ch, tdim, dropout=dropout)\n",
        "\n",
        "        self.down1 = Down(base_ch, base_ch)\n",
        "        self.e2  = ResBlockFiLM(base_ch, 2 * base_ch, tdim, dropout=dropout)\n",
        "        self.e2b = ResBlockFiLM(2 * base_ch, 2 * base_ch, tdim, dropout=dropout)\n",
        "        self.attn_mid = SelfAttention2d(2 * base_ch, num_heads=max(4, (2*base_ch)//64))\n",
        "\n",
        "        self.down2 = Down(2 * base_ch, 2 * base_ch)\n",
        "        self.e3  = ResBlockFiLM(2 * base_ch, 4 * base_ch, tdim, dropout=dropout)\n",
        "        self.e3b = ResBlockFiLM(4 * base_ch, 4 * base_ch, tdim, dropout=dropout)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.mid1 = ResBlockFiLM(4 * base_ch, 4 * base_ch, tdim, dropout=dropout)\n",
        "        self.attn = SelfAttention2d(4 * base_ch, num_heads=4)\n",
        "        self.mid2 = ResBlockFiLM(4 * base_ch, 4 * base_ch, tdim, dropout=dropout)\n",
        "\n",
        "        # Decoder blocks（每尺度两个 block）\n",
        "        self.up1 = Up(4 * base_ch, 2 * base_ch)\n",
        "        self.d2  = ResBlockFiLM(4 * base_ch, 2 * base_ch, tdim, dropout=dropout)\n",
        "        self.d2b = ResBlockFiLM(2 * base_ch, 2 * base_ch, tdim, dropout=dropout)\n",
        "\n",
        "        self.up2 = Up(2 * base_ch, base_ch)\n",
        "        self.d1  = ResBlockFiLM(2 * base_ch, base_ch, tdim, dropout=dropout)\n",
        "        self.d1b = ResBlockFiLM(base_ch, base_ch, tdim, dropout=dropout)\n",
        "\n",
        "        self.out = zero_module(nn.Conv2d(base_ch, 1, 3, padding=1))\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, 1, H, W), t: (B,)\n",
        "        \"\"\"\n",
        "        # 2) forward 里把每尺度再过一次 block\n",
        "        temb = self.time_mlp(self.time_emb(t))\n",
        "\n",
        "        h0 = self.in_conv(x)\n",
        "        h1 = self.e1(h0, temb)\n",
        "        h1 = self.e1b(h1, temb)\n",
        "\n",
        "        h2_in = self.down1(h1)\n",
        "        h2 = self.e2(h2_in, temb)\n",
        "        h2 = self.e2b(h2, temb)\n",
        "        h2 = self.attn_mid(h2)\n",
        "\n",
        "        h3_in = self.down2(h2)\n",
        "        h3 = self.e3(h3_in, temb)\n",
        "        h3 = self.e3b(h3, temb)\n",
        "\n",
        "        h = self.mid1(h3, temb)\n",
        "        h = self.attn(h)\n",
        "        h = self.mid2(h, temb)\n",
        "\n",
        "        h = self.up1(h)\n",
        "        h = torch.cat([h, h2], dim=1)\n",
        "        h = self.d2(h, temb)\n",
        "        h = self.d2b(h, temb)\n",
        "\n",
        "        h = self.up2(h)\n",
        "        h = torch.cat([h, h1], dim=1)\n",
        "        h = self.d1(h, temb)\n",
        "        h = self.d1b(h, temb)\n",
        "\n",
        "        return self.out(F.silu(h))"
      ],
      "metadata": {
        "id": "9Sqe7JFm0H8f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T16:29:58.165925Z",
          "iopub.execute_input": "2025-08-29T16:29:58.166105Z",
          "iopub.status.idle": "2025-08-29T16:29:58.192471Z",
          "shell.execute_reply.started": "2025-08-29T16:29:58.166091Z",
          "shell.execute_reply": "2025-08-29T16:29:58.191834Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decompress"
      ],
      "metadata": {
        "id": "D6JepKaXU4Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# FISTA / LASSO utilities\n",
        "# ==========================================================\n",
        "\n",
        "def soft_threshold(x: torch.Tensor, tau: float) -> torch.Tensor:\n",
        "    \"\"\"Elementwise soft-thresholding: prox_{tau ||.||_1}(x).\"\"\"\n",
        "    return torch.sign(x) * torch.clamp(x.abs() - tau, min=0.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_lipschitz_squared(A: torch.Tensor, iters: int = 50) -> float:\n",
        "    \"\"\"\n",
        "    Estimate L = ||A||_2^2 via power iteration on A^T A.\n",
        "    Safe to run on CPU/GPU. Returns a Python float.\n",
        "    \"\"\"\n",
        "    assert A.ndim == 2, \"A must be 2D (M x D)\"\n",
        "    M, D = A.shape\n",
        "    device = A.device\n",
        "    dtype  = A.dtype\n",
        "\n",
        "    v = torch.randn(D, device=device, dtype=dtype)\n",
        "    v = v / (v.norm() + 1e-12)\n",
        "\n",
        "    for _ in range(iters):\n",
        "        # v <- (A^T A) v / ||(A^T A) v||\n",
        "        Av = A @ v               # (M,)\n",
        "        v  = A.t() @ Av          # (D,)\n",
        "        n  = v.norm()\n",
        "        if n <= 1e-20:\n",
        "            v = torch.randn(D, device=device, dtype=dtype)\n",
        "            v = v / (v.norm() + 1e-12)\n",
        "            continue\n",
        "        v = v / n\n",
        "\n",
        "    # Rayleigh quotient to get sigma_max^2\n",
        "    Av = A @ v\n",
        "    sigma = Av.norm()           # ||A v||_2\n",
        "    L = float(sigma.item() ** 2)\n",
        "    # safety margin to ensure step = 1/L is conservative\n",
        "    return 1.1 * L\n",
        "\n",
        "@torch.no_grad()\n",
        "def fista_lasso_batch(\n",
        "    A: torch.Tensor,      # (M, D)\n",
        "    Y: torch.Tensor,      # (B, M) batched measurements\n",
        "    lam: float,           # lambda in objective\n",
        "    L: float = None,      # Lipschitz constant (||A||_2^2). If None, estimated.\n",
        "    max_iter: int = 500,\n",
        "    tol: float = 1e-5,\n",
        "    verbose: bool = False,\n",
        "    x_init: torch.Tensor = None,   # optional warm start (B, D)\n",
        "):\n",
        "    \"\"\"\n",
        "    Batched FISTA for min_X 0.5||A X^T - Y^T||_F^2 + lam ||X||_1, with rows of X independent.\n",
        "    A: (M, D), Y: (B, M) -> returns X: (B, D)\n",
        "    \"\"\"\n",
        "    assert A.ndim == 2 and Y.ndim == 2, \"A is (M,D), Y is (B,M)\"\n",
        "    M, D = A.shape\n",
        "    B, M2 = Y.shape\n",
        "    assert M == M2, \"Measurement dimension mismatch.\"\n",
        "\n",
        "    device = A.device\n",
        "    dtype  = A.dtype\n",
        "    Y = Y.to(device=device, dtype=dtype)\n",
        "\n",
        "    # Lipschitz constant and step size\n",
        "    if L is None:\n",
        "        L = estimate_lipschitz_squared(A)\n",
        "    step = 1.0 / L\n",
        "\n",
        "    # Initialize\n",
        "    if x_init is None:\n",
        "        X  = torch.zeros(B, D, device=device, dtype=dtype)\n",
        "    else:\n",
        "        X  = x_init.to(device=device, dtype=dtype).clone()\n",
        "    Yk = X.clone()\n",
        "    t  = 1.0\n",
        "\n",
        "    prev_obj = torch.tensor(float(\"inf\"), device=device, dtype=dtype)\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        # Gradient at Yk: grad = (Yk A^T - Y) A  -> shapes: (B,M) @ (M,D) = (B,D)\n",
        "        R = Yk @ A.t() - Y\n",
        "        G = R @ A\n",
        "\n",
        "        # Prox step\n",
        "        X_next = soft_threshold(Yk - step * G, lam * step)\n",
        "        X_next = X_next.clamp(0, 1)   # projection onto [0,1] each iteration\n",
        "\n",
        "        # Nesterov momentum\n",
        "        t_next = 0.5 * (1.0 + torch.sqrt(torch.tensor(1.0 + 4.0 * t * t, device=device, dtype=dtype)))\n",
        "        Yk = X_next + ((t - 1.0) / t_next) * (X_next - X)\n",
        "\n",
        "        # Periodic convergence check (objective)\n",
        "        if (k % 10 == 0) or (k == max_iter - 1):\n",
        "            AX = X_next @ A.t()\n",
        "            res = AX - Y\n",
        "            obj = 0.5 * (res.pow(2).sum(dim=1)).mean() + lam * (X_next.abs().sum(dim=1)).mean()\n",
        "            if verbose:\n",
        "                print(f\"[FISTA] iter={k:04d}  obj={obj.item():.6f}\")\n",
        "            # relative improvement small?\n",
        "            denom = torch.maximum(prev_obj, torch.tensor(1.0, device=device, dtype=dtype))\n",
        "            if torch.isfinite(prev_obj) and torch.abs(prev_obj - obj) <= tol * denom:\n",
        "                X = X_next\n",
        "                break\n",
        "            prev_obj = obj\n",
        "\n",
        "        X, t = X_next, t_next\n",
        "\n",
        "    return X  # (B, D)\n",
        "\n",
        "# ==========================================================\n",
        "# Reconstruction helpers for your compressed loaders\n",
        "# ==========================================================\n",
        "@torch.no_grad()\n",
        "def reconstruct_from_image_loader(\n",
        "    loader_img,           # yields (y_img, label) with y_img shape (B, 1, m, m)\n",
        "    A: torch.Tensor,      # (M, D)\n",
        "    img_shape,            # (C, H, W) of original images\n",
        "    lam: float = 0.01,\n",
        "    L: float = None,\n",
        "    max_iter: int = 500,\n",
        "    tol: float = 1e-5,\n",
        "    clip01: bool = True,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Reconstruct from image-form compressed dataloader (tr/te_loader_c_img).\n",
        "    We just flatten y_img -> y_vec and call the same FISTA.\n",
        "    \"\"\"\n",
        "    C, H, W = img_shape\n",
        "    A_dev = A\n",
        "    device = A_dev.device\n",
        "    dtype  = A_dev.dtype\n",
        "\n",
        "    recons, labels = [], []\n",
        "    for (y_img, lbl) in tqdm(loader_img):\n",
        "        # y_img: (B, 1, m, m) -> (B, M)\n",
        "        y_vec = y_img.view(y_img.size(0), -1).to(device=device, dtype=dtype)\n",
        "        X_hat = fista_lasso_batch(A_dev, y_vec, lam=lam, L=L, max_iter=max_iter, tol=tol, verbose=verbose)\n",
        "        X_hat = X_hat.view(-1, C, H, W)\n",
        "        if clip01:\n",
        "            X_hat = X_hat.clamp(0.0, 1.0)\n",
        "        recons.append(X_hat.cpu())\n",
        "        labels.append(lbl.detach().cpu())\n",
        "    X_imgs = torch.cat(recons, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    return X_imgs, labels"
      ],
      "metadata": {
        "id": "kAkgWf455ly0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T16:29:58.194099Z",
          "iopub.execute_input": "2025-08-29T16:29:58.194328Z",
          "iopub.status.idle": "2025-08-29T16:29:58.210788Z",
          "shell.execute_reply.started": "2025-08-29T16:29:58.194311Z",
          "shell.execute_reply": "2025-08-29T16:29:58.210172Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Example: end-to-end usage\n",
        "# ==========================================================\n",
        "\n",
        "# --- Assume you already have these from your compress & FISTA parts:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "img_shape = tuple(train_ds_raw[0][0].shape)         # (C, H, W)\n",
        "M = SIDE_Y * SIDE_Y\n",
        "A_dev = A_cpu.to(device)\n",
        "L_hat = estimate_lipschitz_squared(A_dev)           # from your FISTA module\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Train on compressed images (UNet)\n",
        "# ---------------------------\n",
        "m = SIDE_Y\n",
        "img_model, img_sde, img_norm = train_score_img(\n",
        "    tr_loader_c_img, te_loader_c_img,\n",
        "    device=device, sde_type=\"VP\",\n",
        "    sigma_min=0.01, sigma_max=50.0,\n",
        "    lr=4e-4, epochs=50, weight_type=\"sigma2\", normalize=True\n",
        ")\n",
        "\n",
        "# 4) Sample compressed images and reconstruct\n",
        "y_samp_img = sample_compressed_img(img_model, img_sde, img_norm, num_samples=64, m_side=m, device=device, steps=1000)\n",
        "X_recon_img = reconstruct_from_compressed_images(y_samp_img, A_dev, img_shape, lam=0.01, L=L_hat, max_iter=700)"
      ],
      "metadata": {
        "id": "bGZ1wKFPVCLc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T16:29:58.211454Z",
          "iopub.execute_input": "2025-08-29T16:29:58.211697Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare original and reconstructed images using MSE"
      ],
      "metadata": {
        "id": "OOYwP2zfLaG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def compute_metrics(gt: torch.Tensor, pred: torch.Tensor) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    gt, pred: (N, C, H, W) in [0,1]\n",
        "    Returns mean metrics across N (also returns per-sample tensors for inspection).\n",
        "    \"\"\"\n",
        "    assert gt.shape == pred.shape, f\"Shape mismatch: {gt.shape} vs {pred.shape}\"\n",
        "    diff = pred - gt\n",
        "    mse_per = (diff ** 2).mean(dim=(1,2,3))  # per-sample MSE\n",
        "    mae_per = diff.abs().mean(dim=(1,2,3))\n",
        "    # PSNR with peak=1.0 for [0,1] images\n",
        "    psnr_per = 10.0 * torch.log10(torch.ones_like(mse_per) / (mse_per + 1e-12))\n",
        "\n",
        "    return {\n",
        "        \"MSE_mean\": float(mse_per.mean().item()),\n",
        "        \"MAE_mean\": float(mae_per.mean().item()),\n",
        "        \"PSNR_mean\": float(psnr_per.mean().item()),\n",
        "        # Optionally expose quantiles for robustness\n",
        "        \"MSE_p95\": float(mse_per.quantile(0.95).item()),\n",
        "        \"PSNR_p05\": float(psnr_per.quantile(0.05).item()),\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_ground_truth(loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    loader yields (x_img, label). Returns:\n",
        "      X_gt: (N, C, H, W), labels: (N,)\n",
        "    Assumes shuffle=False to preserve order.\n",
        "    \"\"\"\n",
        "    xs, ys = [], []\n",
        "    for x, y in loader:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    X_gt = torch.cat(xs, dim=0).contiguous()\n",
        "    labels = torch.cat(ys, dim=0).contiguous()\n",
        "    return X_gt, labels\n",
        "\n",
        "# -----------------------------\n",
        "# End-to-end evaluation helpers\n",
        "# -----------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_reconstruction_image(\n",
        "    te_loader_uncompressed,       # yields (img, label)\n",
        "    te_loader_c_img,              # yields (y_img, label)\n",
        "    A_dev,\n",
        "    img_shape: Tuple[int,int,int],\n",
        "    lam: float,\n",
        "    L: float,\n",
        "    max_iter: int = 500,\n",
        "    tol: float = 1e-5,\n",
        "    verbose: bool = False,\n",
        ") -> Dict[str, float]:\n",
        "    gt_imgs, gt_lbls = collect_ground_truth(te_loader_uncompressed)\n",
        "    Xhat, y_lbls = reconstruct_from_image_loader(\n",
        "        te_loader_c_img, A_dev, img_shape, lam=lam, L=L, max_iter=max_iter, tol=tol, verbose=verbose\n",
        "    )\n",
        "    if not torch.equal(gt_lbls, y_lbls):\n",
        "        print(\"[warn] label misalignment detected (batch sizes or ordering differ). Metrics may be off.\")\n",
        "    return compute_metrics(gt_imgs, Xhat)\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: quick λ sweep on a subset to pick a good starting point\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def lambda_sweep(\n",
        "    te_loader_uncompressed,\n",
        "    te_loader_c_vec,     # or pass te_loader_c_img and swap the recon fn\n",
        "    A_dev,\n",
        "    img_shape,\n",
        "    L: float,\n",
        "    lam_list: List[float] = (0.001, 0.003, 0.01, 0.03, 0.1),\n",
        "    max_iter: int = 500,\n",
        "    max_batches: int = 5,   # keep it small for speed; set None for full eval\n",
        "    use_image_loader: bool = False,\n",
        ") -> List[Tuple[float, Dict[str, float]]]:\n",
        "    # collect a small GT subset\n",
        "    xs, ys = [], []\n",
        "    for b, (x, y) in enumerate(te_loader_uncompressed):\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "        if (max_batches is not None) and (b + 1 >= max_batches):\n",
        "            break\n",
        "    X_gt = torch.cat(xs, dim=0)\n",
        "    Y_gt = torch.cat(ys, dim=0)\n",
        "\n",
        "    # collect a matching subset of compressed batches and reconstruct per λ\n",
        "    results = []\n",
        "    for lam in lam_list:\n",
        "        recons = []\n",
        "        seen = 0\n",
        "        if use_image_loader:\n",
        "            for b, (y_img, lbl) in enumerate(te_loader_c_img):\n",
        "                y_vec = y_img.view(y_img.size(0), -1)\n",
        "                X_hat = fista_lasso_batch(A_dev, y_vec.to(A_dev.device, A_dev.dtype),\n",
        "                                          lam=lam, L=L, max_iter=max_iter)\n",
        "                recons.append(X_hat.cpu())\n",
        "                seen += y_vec.size(0)\n",
        "                if (max_batches is not None) and (b + 1 >= max_batches):\n",
        "                    break\n",
        "        else:\n",
        "            for b, (y_vec, lbl) in enumerate(te_loader_c_vec):\n",
        "                X_hat = fista_lasso_batch(A_dev, y_vec.to(A_dev.device, A_dev.dtype),\n",
        "                                          lam=lam, L=L, max_iter=max_iter)\n",
        "                recons.append(X_hat.cpu())\n",
        "                seen += y_vec.size(0)\n",
        "                if (max_batches is not None) and (b + 1 >= max_batches):\n",
        "                    break\n",
        "\n",
        "        X_hat = torch.cat(recons, dim=0).view(seen, *img_shape).clamp(0,1)\n",
        "        metrics = compute_metrics(X_gt[:seen], X_hat)\n",
        "        results.append((lam, metrics))\n",
        "        print(f\"[λ={lam:.4g}] MSE={metrics['MSE_mean']:.5f}  PSNR={metrics['PSNR_mean']:.3f} dB\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "JHs1qwNxJzXg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = tuple(train_ds_raw[0][0].shape)  # (C,H,W)\n",
        "A_dev = A_cpu.to(device)\n",
        "L_hat = estimate_lipschitz_squared(A_dev)    # from your FISTA module\n",
        "\n",
        "# Image-form compressed test set\n",
        "metrics_img = evaluate_reconstruction_image(\n",
        "    te_loader, te_loader_c_img, A_dev, img_shape, lam=0.003, L=L_hat, max_iter=500\n",
        ")\n",
        "print(\"Image recon:\", metrics_img)\n",
        "\n",
        "# Optional: pick a λ quickly\n",
        "_ = lambda_sweep(\n",
        "    te_loader, te_loader_c_img, A_dev, img_shape, L=L_hat,\n",
        "    lam_list=[0.001, 0.003, 0.01, 0.03, 0.1], max_iter=500, max_batches=100, use_image_loader=True\n",
        ")"
      ],
      "metadata": {
        "id": "Er7JtsuqLR2x",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- helpers to collect the first N items from loaders ----------\n",
        "@torch.no_grad()\n",
        "def _collect_first_n_uncompressed(loader, N):\n",
        "    xs, ys = [], []\n",
        "    for x, y in loader:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "        if sum(t.size(0) for t in xs) >= N:\n",
        "            break\n",
        "    X = torch.cat(xs, dim=0)[:N].contiguous()   # (N, C, H, W)\n",
        "    Y = torch.cat(ys, dim=0)[:N].contiguous()   # (N,)\n",
        "    return X, Y\n",
        "\n",
        "@torch.no_grad()\n",
        "def _collect_first_n_img(loader_img, N):\n",
        "    ys, lbls = [], []\n",
        "    for y_img, lbl in loader_img:\n",
        "        ys.append(y_img)\n",
        "        lbls.append(lbl)\n",
        "        if sum(t.size(0) for t in ys) >= N:\n",
        "            break\n",
        "    Y = torch.cat(ys, dim=0)[:N].contiguous()       # (N, 1, m, m)\n",
        "    L = torch.cat(lbls, dim=0)[:N].contiguous()     # (N,)\n",
        "    return Y, L\n",
        "\n",
        "# ---------- FISTA decode wrappers for N samples ----------\n",
        "@torch.no_grad()\n",
        "def _reconstruct_from_vectors(Y_vec, A, img_shape, lam=0.01, L=None, max_iter=500, tol=1e-5, clamp01=True):\n",
        "    # Y_vec: (N, M); A: (M, D)\n",
        "    device = A.device\n",
        "    dtype  = A.dtype\n",
        "    C, H, W = img_shape\n",
        "    X_hat = fista_lasso_batch(A, Y_vec.to(device=device, dtype=dtype),\n",
        "                              lam=lam, L=L, max_iter=max_iter, tol=tol)  # (N, D)\n",
        "    X_hat = X_hat.view(-1, C, H, W)\n",
        "    if clamp01:\n",
        "        X_hat = X_hat.clamp(0, 1)\n",
        "    return X_hat.cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _reconstruct_from_images(Y_img, A, img_shape, lam=0.01, L=None, max_iter=500, tol=1e-5, clamp01=True):\n",
        "    # Y_img: (N, 1, m, m) -> flatten to (N, M) then decode\n",
        "    Y_vec = Y_img.view(Y_img.size(0), -1)\n",
        "    return _reconstruct_from_vectors(Y_vec, A, img_shape, lam, L, max_iter, tol, clamp01)\n",
        "\n",
        "# ---------- plotting helper ----------\n",
        "def _show_grid(images, title, nrows=4, ncols=4):\n",
        "    \"\"\"\n",
        "    images: (N, C, H, W) in [0,1]\n",
        "    \"\"\"\n",
        "    N, C, H, W = images.shape\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*2.2, nrows*2.2))\n",
        "    fig.suptitle(title, fontsize=14)\n",
        "    for i in range(nrows * ncols):\n",
        "        ax = axes[i // ncols, i % ncols]\n",
        "        ax.axis('off')\n",
        "        if i >= N:\n",
        "            continue\n",
        "        im = images[i]\n",
        "        if C == 1:\n",
        "            ax.imshow(im[0].cpu().numpy(), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        else:\n",
        "            ax.imshow(im.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# ---------- main: collect, reconstruct, visualize ----------\n",
        "@torch.no_grad()\n",
        "def visualize_original_vs_reconstructed(\n",
        "    te_loader, te_loader_c_img,\n",
        "    A_dev, L_hat, img_shape,\n",
        "    lam=0.01, max_iter=500, N=16\n",
        "):\n",
        "    # 1) originals\n",
        "    X_gt, lbl_gt = _collect_first_n_uncompressed(te_loader, N)  # (N,C,H,W)\n",
        "\n",
        "    # 3) recon from image-form compressed\n",
        "    Y_img, lbl_img = _collect_first_n_img(te_loader_c_img, N)   # (N,1,m,m)\n",
        "    X_img = _reconstruct_from_images(Y_img, A_dev, img_shape, lam=lam, L=L_hat, max_iter=max_iter)\n",
        "\n",
        "    # --- quick alignment sanity check ---\n",
        "    if not (torch.equal(lbl_gt, lbl_img)):\n",
        "        print(\"[warn] labels not perfectly aligned across loaders; \"\n",
        "              \"ensure all test loaders are shuffle=False and built from the same base dataset.\")\n",
        "\n",
        "    # 4) visualize\n",
        "    _show_grid(X_gt,  title=\"Original test images (first 16)\")\n",
        "    _show_grid(X_img, title=\"Reconstructed from IMAGE compressed measurements (FISTA)\")\n",
        "\n",
        "# ----------------- run it -----------------\n",
        "# You can tweak lam/max_iter/N as needed\n",
        "visualize_original_vs_reconstructed(\n",
        "    te_loader=te_loader,\n",
        "    te_loader_c_img=te_loader_c_img,\n",
        "    A_dev=A_dev,\n",
        "    L_hat=L_hat,\n",
        "    img_shape=img_shape,\n",
        "    lam=0.003,\n",
        "    max_iter=500,\n",
        "    N=16\n",
        ")"
      ],
      "metadata": {
        "id": "wbQe4N9XLWc3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def _to_images(x: torch.Tensor, img_shape):\n",
        "    \"\"\"Ensure x is (N,C,H,W). If x is (N,D), reshape using img_shape.\"\"\"\n",
        "    if x.dim() == 2:\n",
        "        C, H, W = img_shape\n",
        "        x = x.view(x.size(0), C, H, W)\n",
        "    return x\n",
        "\n",
        "def _show_grid(images: torch.Tensor, title: str, n: int = 16, ncols: int = 4):\n",
        "    images = images.cpu()\n",
        "    n = min(n, images.size(0))\n",
        "    nrows = (n + ncols - 1) // ncols\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(2.2*ncols, 2.2*nrows))\n",
        "    axes = axes.flatten() if isinstance(axes, (list, tuple)) else axes.ravel()\n",
        "\n",
        "    for i in range(n):\n",
        "        ax = axes[i]; ax.axis('off')\n",
        "        img = images[i]\n",
        "        if img.size(0) == 1:   # grayscale\n",
        "            ax.imshow(img[0].numpy(), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "        else:                  # RGB\n",
        "            ax.imshow(img.permute(1,2,0).numpy())\n",
        "    for j in range(n, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    fig.suptitle(title, fontsize=12)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# ---- Use it ----\n",
        "# img_shape must be your original image shape, e.g. (1, 32, 32) or (1, 28, 28)\n",
        "# 4) Sample compressed images and reconstruct\n",
        "X_recon_img_imgs = _to_images(X_recon_img, img_shape)  # should already be (N,C,H,W)\n",
        "\n",
        "_show_grid(X_recon_img_imgs, \"Reconstruction from IMAGE measurements (FISTA)\",  n=36, ncols=6)"
      ],
      "metadata": {
        "id": "fbEDb9Hqo6-8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_compressed_img(\n",
        "    model, sde, normalizer, num_samples, m_side, device,\n",
        "    steps=1000, snr=0.15, use_pc=True\n",
        "):\n",
        "    \"\"\"Return (N,1,side,side) in compressed or image space \"\"\"\n",
        "    model.eval()\n",
        "    x = sde.prior_sample((num_samples, 1, m_side, m_side), device=device)  # t=1\n",
        "\n",
        "    t_grid = torch.linspace(1.0, 0.0, steps+1, device=device)\n",
        "    for i in tqdm(range(steps)):\n",
        "        t, t_next = t_grid[i], t_grid[i+1]\n",
        "        dt = (t_next - t)  # 负数\n",
        "        tb = torch.full((num_samples,), float(t.item()), device=device)\n",
        "\n",
        "        s = model(x, tb)  # score(x,t)\n",
        "\n",
        "        beta = sde.g2(tb).view(-1,1,1,1)          # = β(t)\n",
        "        drift = -0.5 * beta * x - beta * s\n",
        "        diff  = torch.sqrt(torch.clamp(beta * (-dt), min=1e-12))\n",
        "\n",
        "        # Predictor: Euler–Maruyama\n",
        "        z = torch.randn_like(x)\n",
        "        x = x + drift * dt + diff * z\n",
        "\n",
        "        if use_pc:\n",
        "            # Corrector: Langevin steps（\n",
        "            with torch.no_grad():\n",
        "                noise_std = diff.mean()\n",
        "                score_norm = torch.sqrt((s**2).mean(dim=(1,2,3), keepdim=True) + 1e-12)\n",
        "                step_size = (snr * noise_std / (score_norm + 1e-12))**2\n",
        "                for _ in range(1):\n",
        "                    zc = torch.randn_like(x)\n",
        "                    x = x + step_size * s + torch.sqrt(2.0 * step_size) * zc\n",
        "\n",
        "    if normalizer.mean is not None:\n",
        "        x = normalizer.denormalize(x)\n",
        "    return x.clamp(-1, 1)\n",
        "\n",
        "y_samp_img = sample_compressed_img(img_model, img_sde, img_norm, num_samples=64, m_side=m, device=device, steps=1000)\n",
        "X_recon_img = reconstruct_from_compressed_images(y_samp_img, A_dev, img_shape, lam=0.05, L=L_hat, max_iter=1000)\n",
        "X_recon_img_imgs = _to_images(X_recon_img, img_shape)  # should already be (N,C,H,W)\n",
        "_show_grid(X_recon_img_imgs, \"Reconstruction from IMAGE measurements (FISTA)\",  n=49, ncols=7)"
      ],
      "metadata": {
        "id": "Z9mlwD2cyTwI",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_compressed_img_pf_heun(\n",
        "    model, sde, normalizer, num_samples, m_side, device, steps=1200\n",
        "):\n",
        "    model.eval()\n",
        "    x = sde.prior_sample((num_samples,1,m_side,m_side), device=device)\n",
        "    s = torch.linspace(0, 1, steps+1, device=device)\n",
        "    t_grid = (s**2).flip(0) * (1 - 1e-5) + 1e-5\n",
        "\n",
        "    for i in tqdm(range(steps)):\n",
        "        t1, t2 = t_grid[i], t_grid[i+1]\n",
        "        dt = t2 - t1\n",
        "\n",
        "        tb1  = torch.full((num_samples,), float(t1.item()), device=device)\n",
        "        beta1 = sde.g2(tb1).view(-1,1,1,1)\n",
        "        s1    = model(x, tb1)\n",
        "\n",
        "        sigma1 = sde.sigma(tb1).view(-1,1,1,1)\n",
        "        s1n = torch.sqrt((s1*s1).mean(dim=(1,2,3), keepdim=True) + 1e-12)\n",
        "        maxn = 5.0 / (sigma1 + 1e-12)\n",
        "        s1   = s1 * torch.minimum(torch.ones_like(s1n), maxn/s1n)\n",
        "\n",
        "        f1 = -0.5 * beta1 * (x + s1)\n",
        "        x_e = x + f1 * dt\n",
        "\n",
        "        tb2   = torch.full((num_samples,), float(t2.item()), device=device)\n",
        "        beta2 = sde.g2(tb2).view(-1,1,1,1)\n",
        "        s2    = model(x_e, tb2)\n",
        "        f2 = -0.5 * beta2 * (x_e + s2)\n",
        "\n",
        "        x = x + 0.5 * (f1 + f2) * dt\n",
        "\n",
        "    if normalizer.mean is not None:\n",
        "        x = normalizer.denormalize(x)\n",
        "    return x.clamp(-1, 1)\n",
        "\n",
        "y_samp_img = sample_compressed_img_pf_heun(img_model, img_sde, img_norm, num_samples=64, m_side=m, device=device, steps=1000)\n",
        "X_recon_img = reconstruct_from_compressed_images(y_samp_img, A_dev, img_shape, lam=0.05, L=L_hat, max_iter=700)\n",
        "X_recon_img_imgs = _to_images(X_recon_img, img_shape)  # should already be (N,C,H,W)\n",
        "_show_grid(X_recon_img_imgs, \"Reconstruction from IMAGE measurements (FISTA)\",  n=49, ncols=7)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SszaIYJk8n8X"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}